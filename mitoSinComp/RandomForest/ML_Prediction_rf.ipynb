{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BoBFPTKtXrM"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "import seaborn as sns\n",
        "#import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Real\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper around train_test_split for convenience\n",
        "def traintestsplit(df, random_state=0):\n",
        "    X = df.drop(['cc_pixel_intensity_ratio', 'line_id'],axis=1)\n",
        "    Y = df['cc_pixel_intensity_ratio']\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=random_state)\n",
        "    return X_train, X_test, Y_train, Y_test"
      ],
      "metadata": {
        "id": "dgZ2HseFtaJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this cell loads dataframes and keeps only features of interest\n",
        "# each dataframe has the naming convention\n",
        "# group_type_df where type may be networked or standalones (i.e. non-networked)\n",
        "# we divide the last four groups into net and standalones in a different cell below\n",
        "\n",
        "# Note: Replace * with actual file path\n",
        "# Name of the Dataframe as per name of the experimental group\n",
        "# format: groupname_(net or standalone)_df\n",
        "tol1hr_standalones_df = pd.read_csv('*/tol1hr_standalones.csv', index_col=None)\n",
        "tol1hr_standalones_df.drop(labels=tol1hr_standalones_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "tol1hr_net_df = pd.read_csv('*/tol1hr_net.csv', index_col=None)\n",
        "tol1hr_net_df.drop(labels=tol1hr_net_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "tol4_standalones_df = pd.read_csv('*/tol4_standalones.csv', index_col=None)\n",
        "tol4_standalones_df.drop(labels=tol4_standalones_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "tol4_net_df = pd.read_csv('*/tol4_net.csv', index_col=None)\n",
        "tol4_net_df.drop(labels=tol4_net_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD10nM1_standalones_df = pd.read_csv('*/TCDD10nM1_standalones.csv', index_col=None)\n",
        "TCDD10nM1_standalones_df.drop(labels=TCDD10nM1_standalones_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD10nM1_net_df = pd.read_csv('*/TCDD10nM1_net.csv', index_col=None)\n",
        "TCDD10nM1_net_df.drop(labels=TCDD10nM1_net_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD10nM4_standalones_df = pd.read_csv('*/TCDD10nM4_standalones.csv', index_col=None)\n",
        "TCDD10nM4_standalones_df.drop(labels=TCDD10nM4_standalones_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD10nM4_net_df = pd.read_csv('*/TCDD10nM4_net.csv', index_col=None)\n",
        "TCDD10nM4_net_df.drop(labels=TCDD10nM4_net_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD1nM1_standalones_df = pd.read_csv('*/TCDD1nM1_standalones.csv', index_col=None)\n",
        "TCDD1nM1_standalones_df.drop(labels=TCDD1nM1_standalones_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD1nM1_net_df = pd.read_csv('*/TCDD1nM1_net.csv', index_col=None)\n",
        "TCDD1nM1_net_df.drop(labels=TCDD1nM1_net_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD1nM4_standalones_df = pd.read_csv('*/TCDD1nM4_standalones.csv', index_col=None)\n",
        "TCDD1nM4_standalones_df.drop(labels=TCDD1nM4_standalones_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TCDD1nM4_net_df = pd.read_csv('*/TCDD1nM4_net.csv', index_col=None)\n",
        "TCDD1nM4_net_df.drop(labels=TCDD1nM4_net_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "TO_df = pd.read_csv('*/TO_sheet.csv', index_col=None)\n",
        "TO_df.drop(labels=TO_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "_1n_df = pd.read_csv('*/1n_sheet.csv', index_col=None)\n",
        "_1n_df.drop(labels=_1n_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks','normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "_10_df = pd.read_csv('*/10_sheet.csv', index_col=None)\n",
        "_10_df.drop(labels=_10_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)\n",
        "\n",
        "DS_df = pd.read_csv('*/DS_sheet.csv', index_col=None)\n",
        "DS_df.drop(labels=DS_df.columns.difference(['cc_length_(um)', 'nodes', 'edges', 'cc_pixel_intensity_ratio', 'cc_max_PK', 'line_id', 'diameter', 'element_length_(um)', 'normalized_length_by_networks', 'normalized_length_by_standalones', 'edge_density', 'node_density']), axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "oeyFbIVM26JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we drop duplicates based on the element_length column\n",
        "tol4_net_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "tol1hr_net_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD10nM1_net_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD10nM4_net_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD1nM1_net_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD1nM4_net_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "tol4_standalones_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "tol1hr_standalones_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD10nM1_standalones_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD10nM4_standalones_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD1nM1_standalones_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TCDD1nM4_standalones_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "TO_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "_1n_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "_10_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)\n",
        "DS_df.drop_duplicates(subset=['element_length_(um)'], inplace=True, ignore_index=True)"
      ],
      "metadata": {
        "id": "DyG1OwMh291k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# splitting the last four cells into net and standalones\n",
        "# if there's only two nodes and one edge, that's a standalone\n",
        "# Note: This step depends on how your data is grouped, if all data is grouped together, the following code needs to be\n",
        "# executed for all experimental groups\n",
        "\n",
        "TO_standalones_df = TO_df[((TO_df['nodes'] == 2) & (TO_df['edges'] == 1))].copy()\n",
        "TO_standalones_df.drop(labels=['normalized_length_by_networks'], axis=1, inplace=True)\n",
        "TO_net_df = TO_df[~((TO_df['nodes'] == 2) & (TO_df['edges'] == 1))].copy()\n",
        "TO_net_df.drop(labels=['normalized_length_by_standalones'], axis=1, inplace=True)\n",
        "\n",
        "_1n_standalones_df = _1n_df[((_1n_df['nodes'] == 2) & (_1n_df['edges'] == 1))].copy()\n",
        "_1n_standalones_df.drop(labels=['normalized_length_by_networks'], axis=1, inplace=True)\n",
        "_1n_net_df = _1n_df[~((_1n_df['nodes'] == 2) & (_1n_df['edges'] == 1))].copy()\n",
        "_1n_net_df.drop(labels=['normalized_length_by_standalones'], axis=1, inplace=True)\n",
        "\n",
        "_10_standalones_df = _10_df[((_10_df['nodes'] == 2) & (_10_df['edges'] == 1))].copy()\n",
        "_10_standalones_df.drop(labels=['normalized_length_by_networks'], axis=1, inplace=True)\n",
        "_10_net_df = _10_df[~((_10_df['nodes'] == 2) & (_10_df['edges'] == 1))].copy()\n",
        "_10_net_df.drop(labels=['normalized_length_by_standalones'], axis=1, inplace=True)\n",
        "\n",
        "DS_standalones_df = DS_df[((DS_df['nodes'] == 2) & (DS_df['edges'] == 1))].copy()\n",
        "DS_standalones_df.drop(labels=['normalized_length_by_networks'], axis=1, inplace=True)\n",
        "DS_net_df = DS_df[~((DS_df['nodes'] == 2) & (DS_df['edges'] == 1))].copy()\n",
        "DS_net_df.drop(labels=['normalized_length_by_standalones'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "sO10MPbyuSB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we create the pooled datasets for net and standalones\n",
        "standalones_pooled_df = pd.concat([tol1hr_standalones_df, tol4_standalones_df, TCDD10nM1_standalones_df, TCDD10nM4_standalones_df, TCDD1nM1_standalones_df, TCDD1nM4_standalones_df, TO_standalones_df, _1n_standalones_df, _10_standalones_df, DS_standalones_df], axis=0, ignore_index=True)\n",
        "net_pooled_df = pd.concat([tol1hr_net_df, tol4_net_df, TCDD10nM1_net_df, TCDD10nM4_net_df, TCDD1nM1_net_df, TCDD1nM4_net_df, TO_net_df, _1n_net_df, _10_net_df, DS_net_df], axis=0, ignore_index=True)"
      ],
      "metadata": {
        "id": "HtznHsNruTiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we apply a log-transformation for all length related features\n",
        "net_pooled_df['normalized_length_by_networks'] = np.log(net_pooled_df['normalized_length_by_networks'])\n",
        "standalones_pooled_df['normalized_length_by_standalones'] = np.log(standalones_pooled_df['normalized_length_by_standalones'])\n",
        "net_pooled_df['element_length_(um)'] = np.log(net_pooled_df['element_length_(um)'])\n",
        "standalones_pooled_df['element_length_(um)'] = np.log(standalones_pooled_df['element_length_(um)'])\n",
        "net_pooled_df['cc_length_(um)'] = np.log(net_pooled_df['cc_length_(um)'])\n",
        "standalones_pooled_df['cc_length_(um)'] = np.log(standalones_pooled_df['cc_length_(um)'])"
      ],
      "metadata": {
        "id": "mAuh1vT_utIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# drops nans (artifact of log transform/naturally occuring) from data\n",
        "net_pooled_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "net_pooled_df.dropna(how=\"all\", inplace=True)\n",
        "\n",
        "standalones_pooled_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "standalones_pooled_df.dropna(how=\"all\", inplace=True)"
      ],
      "metadata": {
        "id": "ab5SCpmPuxGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# we filter out the outliers\n",
        "net_pooled_df = net_pooled_df[net_pooled_df['cc_pixel_intensity_ratio'] <= 0.5]\n",
        "standalones_pooled_df = standalones_pooled_df[standalones_pooled_df['cc_pixel_intensity_ratio'] <= 0.5]"
      ],
      "metadata": {
        "id": "ix8nsY1_u1nz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Models"
      ],
      "metadata": {
        "id": "_r0__vQlvEqo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we select model hyperparams using the skopt library\n",
        "# read docs here: https://scikit-optimize.github.io/stable/modules/generated/skopt.BayesSearchCV.html\n",
        "# for this magnitude of hyperparameter values, there are only marginal differences for parameters that are tuned slightly differently\n",
        "# so i've only included one example of this function call here\n",
        "# its also the most time consuming function to call in the entire pipeline\n",
        "X_train, X_test, y_train, y_test = traintestsplit(net_pooled_df)\n",
        "\n",
        "net_pooled_opt = BayesSearchCV(\n",
        "    RandomForestRegressor(),\n",
        "    {\n",
        "        'n_estimators': (70,100),\n",
        "        'max_depth': (30,50),\n",
        "        'ccp_alpha': Real(0,0.5),\n",
        "    },\n",
        "    n_iter = 50\n",
        ")\n",
        "\n",
        "net_pooled_opt.fit(X_train, y_train)\n",
        "print(net_pooled_opt.score(X_test, y_test))\n",
        "print(net_pooled_opt.best_params_)"
      ],
      "metadata": {
        "id": "CY2skHdLwA8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrapper around 5-fold CV and fitting random forests\n",
        "\n",
        "def evaluate_model(data, model):\n",
        "    X_train, X_test, y_train, y_test = traintestsplit(\n",
        "        data, random_state = 0\n",
        "    )\n",
        "\n",
        "    cv_scores = cross_val_score(\n",
        "        model, X_train, y_train,\n",
        "        cv=5, scoring='r2'\n",
        "    )\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_score = model.score(X_train, y_train)\n",
        "    test_score = model.score(X_test, y_test)\n",
        "\n",
        "    return {\n",
        "        'cv_scores': cv_scores,\n",
        "        'cv_mean': cv_scores.mean(),\n",
        "        'cv_std': cv_scores.std(),\n",
        "        'train_score': train_score,\n",
        "        'test_score': test_score\n",
        "    }"
      ],
      "metadata": {
        "id": "s5MDIrVFvHFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next few cells run the function above on the various dataframes and are segregated by type"
      ],
      "metadata": {
        "id": "tT74nkPbvXxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"standalones_pooled_df\")\n",
        "standalones_pooled_rf = RandomForestRegressor(max_depth=35,n_estimators=75, criterion='squared_error')\n",
        "results = evaluate_model(standalones_pooled_df, standalones_pooled_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"net_pooled_df\")\n",
        "net_pooled_rf = RandomForestRegressor(max_depth=35,n_estimators=75, criterion='squared_error')\n",
        "results = evaluate_model(net_pooled_df, net_pooled_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")"
      ],
      "metadata": {
        "id": "btSZv5sqvVyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TCDD1nM4_net_df\")\n",
        "TCDD1nM4_rf = RandomForestRegressor(max_depth=44,n_estimators=105, criterion='squared_error')\n",
        "results = evaluate_model(TCDD1nM4_net_df, TCDD1nM4_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TCDD1nM1_net_df\")\n",
        "TCDD1nM1_rf = RandomForestRegressor(max_depth=43,n_estimators=105, criterion='squared_error')\n",
        "results = evaluate_model(TCDD1nM1_net_df, TCDD1nM1_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TCDD10nM4_net_df\")\n",
        "TCDD10nM4_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(TCDD10nM4_net_df, TCDD10nM4_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TCDD10nM1_net_df\")\n",
        "TCDD10nM1_rf = RandomForestRegressor(max_depth=45,n_estimators=98, criterion='squared_error')\n",
        "results = evaluate_model(TCDD1nM1_net_df, TCDD1nM1_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"tol4_net_df\")\n",
        "tol4_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(tol4_net_df, tol4_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"tol1hr_net_df\")\n",
        "tol1hr_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(tol1hr_net_df, tol1hr_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")"
      ],
      "metadata": {
        "id": "23gGlEvovquR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DS_net_df\")\n",
        "DS_net_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(DS_net_df, DS_net_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TO__net_df\")\n",
        "TO__net_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(TO_net_df, TO__net_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"_1n_net_df\")\n",
        "_1n_net_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(_1n_net_df, _1n_net_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"_10_net_df\")\n",
        "_10_net_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(_10_net_df, _10_net_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")"
      ],
      "metadata": {
        "id": "LU_JAfNFvsaY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"TCDD1nM4_standalones_df\")\n",
        "TCDD1nM4_rf = RandomForestRegressor(max_depth=44,n_estimators=105, criterion='squared_error')\n",
        "results = evaluate_model(TCDD1nM4_standalones_df, TCDD1nM4_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TCDD1nM1_standalones_df\")\n",
        "TCDD1nM1_rf = RandomForestRegressor(max_depth=43,n_estimators=105, criterion='squared_error')\n",
        "results = evaluate_model(TCDD1nM1_standalones_df, TCDD1nM1_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TCDD10nM4_standalones_df\")\n",
        "TCDD10nM4_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(TCDD10nM4_standalones_df, TCDD10nM4_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TCDD10nM1_standalones_df\")\n",
        "TCDD10nM1_rf = RandomForestRegressor(max_depth=45,n_estimators=98, criterion='squared_error')\n",
        "results = evaluate_model(TCDD1nM1_standalones_df, TCDD1nM1_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"tol4_standalones_df\")\n",
        "tol4_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(tol4_standalones_df, tol4_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"tol1hr_standalones_df\")\n",
        "tol1hr_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(tol1hr_standalones_df, tol1hr_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")"
      ],
      "metadata": {
        "id": "qmfxs3Ldvw4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"DS_standalones_df\")\n",
        "DS_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(DS_standalones_df, DS_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"TO_standalones_df\")\n",
        "TO__rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(TO_standalones_df, TO__rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"_1n_standalones_df\")\n",
        "_1n_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(_1n_standalones_df, _1n_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")\n",
        "\n",
        "print(\"_10_standalones_df\")\n",
        "_10_rf = RandomForestRegressor(max_depth=45,n_estimators=96, criterion='squared_error')\n",
        "results = evaluate_model(_10_standalones_df, _10_rf)\n",
        "print(\"Cross-validation scores:\", results['cv_scores'])\n",
        "print(f\"CV mean: {results['cv_mean']:.3f} (+/- {results['cv_std']*2:.3f})\")\n",
        "print(f\"Training score: {results['train_score']:.3f}\")\n",
        "\n",
        "print(f\"Test score: {results['test_score']:.3f}\")"
      ],
      "metadata": {
        "id": "1I2HGX23vyn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "AGOCfEXKwr1m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = traintestsplit(net_pooled_df)\n",
        "importances = net_pooled_rf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.barh(range(len(importances)), importances[indices])\n",
        "ax.set_yticks(range(len(importances)))\n",
        "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
      ],
      "metadata": {
        "id": "aLdB6Hgtv9Mf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = traintestsplit(standalones_pooled_df)\n",
        "importances = standalones_pooled_rf.feature_importances_\n",
        "indices = np.argsort(importances)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.barh(range(len(importances)), importances[indices])\n",
        "ax.set_yticks(range(len(importances)))\n",
        "_ = ax.set_yticklabels(np.array(X_train.columns)[indices])"
      ],
      "metadata": {
        "id": "-k4_WWfkwvPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Size Reduction Experiment\n",
        "\n",
        "Can we attribute better performance on net_pooled_df to the fact that it simply has more rows of data? The experiment below gradually reduces the number of rows in net_pooled_df till it matches the size of standalones_pooled_df and plots the reduction in $R^2$"
      ],
      "metadata": {
        "id": "xwRHshYzw4Hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reduce_and_score(df, target_size, step=3000):\n",
        "    scores = []\n",
        "    sizes = []\n",
        "    while len(df) > target_size:\n",
        "        df = df.sample(frac=1).iloc[:-step].reset_index(drop=True)\n",
        "        sizes.append(len(df))\n",
        "\n",
        "        X_train, X_test, Y_train, Y_test = traintestsplit(df)\n",
        "        rf = RandomForestRegressor(max_depth=45, n_estimators=96, criterion='squared_error')\n",
        "        rf.fit(X_train, Y_train)\n",
        "        scores.append(rf.score(X_test, Y_test))\n",
        "    return sizes, scores\n",
        "\n",
        "target_size = len(standalones_pooled_df)\n",
        "sizes, scores = reduce_and_score(net_pooled_df, target_size)\n",
        "\n",
        "# we also train on standalones to get a reference point\n",
        "X_train_sa, X_test_sa, Y_train_sa, Y_test_sa = traintestsplit(standalones_pooled_df)\n",
        "rf_sa = RandomForestRegressor(max_depth=45, n_estimators=96, criterion='squared_error')\n",
        "rf_sa.fit(X_train_sa, Y_train_sa)\n",
        "standalone_score = rf_sa.score(X_test_sa, Y_test_sa)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.plot(sizes[::-1], scores[::-1], marker='o', label='Reduced Dataset')\n",
        "\n",
        "plt.scatter(target_size, standalone_score, color='red', s=100, marker='*',\n",
        "            label='Standalone Model')\n",
        "\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.gca().invert_xaxis()\n",
        "plt.ylabel('R² Score')\n",
        "plt.title('R² Score vs. Dataset Size (Random Removal)')\n",
        "plt.grid(True)\n",
        "plt.ylim(0, 1)\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gKe-ZP84w321"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plots of Error Distribution and Target Feature Distribution"
      ],
      "metadata": {
        "id": "1ZALZdkBxgEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regression_dot_plot(y_pred, y_test, color):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_test, y_pred, s=18, c=color, alpha=0.8)\n",
        "\n",
        "    max_val = max(np.max(y_test), np.max(y_pred))\n",
        "    plt.plot([0, max_val], [0, max_val], 'k--', alpha=0.8)\n",
        "\n",
        "    plt.xlabel('True Values', fontsize=24, labelpad=15)\n",
        "    plt.ylabel('Predictions', fontsize=24, labelpad=15)\n",
        "    plt.title('Regression Dot Plot', fontsize=24, pad=20)\n",
        "    plt.tick_params(axis='both', which='major', labelsize=24)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Mn23QBH7mlR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def error_versus_frequency_plot(y_pred, Y_test, color):\n",
        "    errors = np.abs(y_pred - Y_test)\n",
        "\n",
        "    num_bins = int((1/4)*np.sqrt(len(Y_test)))\n",
        "    bins = np.linspace(Y_test.min(), Y_test.max(), num_bins)\n",
        "    bin_indices = np.digitize(Y_test, bins)\n",
        "\n",
        "    bin_errors = [errors[bin_indices == i] for i in range(1, num_bins)]\n",
        "    avg_errors = [np.mean(be) if len(be) > 0 else 0 for be in bin_errors]\n",
        "\n",
        "    bin_centers = (bins[:-1] + bins[1:]) / 2\n",
        "\n",
        "    bin_counts = [np.sum(bin_indices == i) for i in range(1, num_bins)]\n",
        "\n",
        "    max_count = max(bin_counts) if bin_counts else 1\n",
        "    max_error = max(avg_errors) if avg_errors else 1\n",
        "    normalized_counts = [count * max_error / max_count for count in bin_counts]\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    plt.bar(bin_centers, avg_errors, width=(bins[1] - bins[0]) * 0.9,\n",
        "            edgecolor=color, linewidth=2, fill=False,\n",
        "            label='Mean Absolute Error')\n",
        "\n",
        "    plt.plot(bin_centers, normalized_counts, color=color, linewidth=3,\n",
        "             marker='o', markersize=8, label='Target Value Distribution (scaled)')\n",
        "\n",
        "    plt.tick_params(axis='both', which='major', labelsize=24)\n",
        "\n",
        "    plt.xlabel('Binned Target Value', labelpad=15)\n",
        "    plt.ylabel('Mean Absolute Prediction Error', labelpad=15)\n",
        "    plt.title('Mean Absolute Error and Target Value Distribution')\n",
        "    plt.legend()\n",
        "\n",
        "    ax2 = plt.gca().twinx()\n",
        "    ax2.set_ylabel('Percentage of Test Set', labelpad=15)\n",
        "    ax2.set_ylim(0, 25)\n",
        "\n",
        "    scaling_factor = max_error / (25 * max_count / len(Y_test))\n",
        "    ax2.plot(bin_centers, [count * scaling_factor for count in bin_counts], alpha=0)\n",
        "    ax2.tick_params(axis='y', labelsize=24, pad=20)\n",
        "\n",
        "    plt.gca().tick_params(axis='both', pad=20)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Vp2ReD1emOB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = traintestsplit(\n",
        "        net_pooled_df\n",
        "    )\n",
        "y_pred = net_pooled_rf.predict(X_test)\n",
        "print(len(X_test))\n",
        "print(net_pooled_rf.score(X_test, y_test))\n",
        "regression_dot_plot(y_pred, y_test, '#CC0000')\n",
        "error_versus_frequency_plot(y_pred, y_test, '#CC0000')\n",
        "\n",
        "X_train, X_test, y_train, y_test = traintestsplit(\n",
        "        standalones_pooled_df\n",
        "    )\n",
        "y_pred = standalones_pooled_rf.predict(X_test)\n",
        "print(len(X_test))\n",
        "print(standalones_pooled_rf.score(X_test, y_test))\n",
        "regression_dot_plot(y_pred, y_test, 'black')\n",
        "error_versus_frequency_plot(y_pred, y_test, 'black')"
      ],
      "metadata": {
        "id": "Q8PYSjA8xogV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}